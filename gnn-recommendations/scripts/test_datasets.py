"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤.

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
1. –ó–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö
2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —Ä–µ–π—Ç–∏–Ω–≥–æ–≤
3. –ì–ª–æ–±–∞–ª—å–Ω—ã–π 80-10-10 split
4. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–æ–≤
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir / "src"))

from data.dataset import RecommendationDataset


def test_dataset(dataset_name: str):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞."""
    print(f"\n{'#'*80}")
    print(f"# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_name}")
    print(f"{'#'*80}\n")

    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        dataset = RecommendationDataset(
            name=dataset_name,
            root_dir=str(root_dir)
        )

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        print("\n--- –≠—Ç–∞–ø 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---")
        dataset.load_raw_data()

        print("\n--- –≠—Ç–∞–ø 2: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ ---")
        dataset.preprocess()

        print("\n--- –≠—Ç–∞–ø 3: –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ train/val/test ---")
        dataset.split()

        print("\n--- –≠—Ç–∞–ø 4: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ ---")
        dataset.build_graph()

        # –í—ã–≤–æ–¥–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"\n{'='*60}")
        print(f"–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê: {dataset_name}")
        print(f"{'='*60}")
        print(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {dataset.n_users}")
        print(f"–ê–π—Ç–µ–º–æ–≤: {dataset.n_items}")
        print(f"Train: {len(dataset.train_data)} ({len(dataset.train_data)/len(dataset.processed_data)*100:.1f}%)")
        print(f"Valid: {len(dataset.valid_data)} ({len(dataset.valid_data)/len(dataset.processed_data)*100:.1f}%)")
        print(f"Test: {len(dataset.test_data)} ({len(dataset.test_data)/len(dataset.processed_data)*100:.1f}%)")
        print(f"–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å: {dataset.stats['sparsity']:.6f}")
        print(f"{'='*60}\n")

        print(f"‚úì –î–∞—Ç–∞—Å–µ—Ç {dataset_name} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω!")
        return True

    except Exception as e:
        print(f"\n‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ {dataset_name}:")
        print(f"  {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã."""
    datasets = [
        'ml-100k',      # MovieLens 100k
        'ml-1m',        # MovieLens 1M
        'facebook',     # Facebook
        # 'amazon_books', # Amazon Books (–±–æ–ª—å—à–æ–π —Ñ–∞–π–ª, –º–æ–∂–Ω–æ –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞)
    ]

    results = {}

    for dataset_name in datasets:
        success = test_dataset(dataset_name)
        results[dataset_name] = success

    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print(f"\n{'#'*80}")
    print("# –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    print(f"{'#'*80}\n")

    for dataset_name, success in results.items():
        status = "‚úì OK" if success else "‚úó FAIL"
        print(f"{status} - {dataset_name}")

    total = len(results)
    passed = sum(results.values())
    print(f"\n–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total}")
    print(f"–ü—Ä–æ–π–¥–µ–Ω–æ: {passed}")
    print(f"–ü—Ä–æ–≤–∞–ª–µ–Ω–æ: {total - passed}")

    if passed == total:
        print("\nüéâ –í—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
    else:
        print("\n‚ö†Ô∏è  –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –Ω–µ –ø—Ä–æ—à–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫—É.")


if __name__ == "__main__":
    main()
