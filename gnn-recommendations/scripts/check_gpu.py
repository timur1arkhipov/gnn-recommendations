"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ GPU.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/check_gpu.py
"""

import torch
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src
script_dir = Path(__file__).resolve().parent
project_root = script_dir.parent
sys.path.insert(0, str(project_root / "src"))


def check_cuda_available():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA."""
    print("=" * 60)
    print("–ü–†–û–í–ï–†–ö–ê CUDA")
    print("=" * 60)
    
    is_available = torch.cuda.is_available()
    print(f"\n‚úì CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {is_available}")
    
    if not is_available:
        print("\n‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞!")
        print("\n–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("1. PyTorch —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –±–µ–∑ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ CUDA")
        print("2. –î—Ä–∞–π–≤–µ—Ä—ã NVIDIA –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
        print("3. GPU –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç CUDA")
        print("\n–£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch —Å CUDA:")
        print("  pip uninstall torch torchvision torchaudio")
        print("  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        return False
    
    return True


def check_cuda_info():
    """–í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ CUDA."""
    print("\n" + "=" * 60)
    print("–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û CUDA")
    print("=" * 60)
    
    print(f"\n‚úì –í–µ—Ä—Å–∏—è CUDA: {torch.version.cuda}")
    print(f"‚úì –í–µ—Ä—Å–∏—è cuDNN: {torch.backends.cudnn.version()}")
    print(f"‚úì –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {torch.cuda.device_count()}")


def check_gpu_info():
    """–í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU."""
    print("\n" + "=" * 60)
    print("–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û GPU")
    print("=" * 60)
    
    device_count = torch.cuda.device_count()
    
    for i in range(device_count):
        print(f"\nüéÆ GPU {i}:")
        print(f"  –ù–∞–∑–≤–∞–Ω–∏–µ: {torch.cuda.get_device_name(i)}")
        
        props = torch.cuda.get_device_properties(i)
        print(f"  Compute Capability: {props.major}.{props.minor}")
        print(f"  –û–±—â–∞—è –ø–∞–º—è—Ç—å: {props.total_memory / 1024**3:.2f} GB")
        print(f"  Multiprocessors: {props.multi_processor_count}")
        
        # –¢–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
        memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
        memory_reserved = torch.cuda.memory_reserved(i) / 1024**3
        print(f"  –ü–∞–º—è—Ç—å –≤—ã–¥–µ–ª–µ–Ω–∞: {memory_allocated:.2f} GB")
        print(f"  –ü–∞–º—è—Ç—å –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–∞: {memory_reserved:.2f} GB")


def test_simple_operation():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ—Å—Ç—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞ GPU."""
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ –ü–†–û–°–¢–´–• –û–ü–ï–†–ê–¶–ò–ô")
    print("=" * 60)
    
    device = torch.device('cuda:0')
    print(f"\n‚úì –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä—ã –Ω–∞ GPU
    print("\n1. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤ –Ω–∞ GPU...")
    x = torch.randn(1000, 1000, device=device)
    y = torch.randn(1000, 1000, device=device)
    print(f"   ‚úì –¢–µ–Ω–∑–æ—Ä x: shape={x.shape}, device={x.device}")
    print(f"   ‚úì –¢–µ–Ω–∑–æ—Ä y: shape={y.shape}, device={y.device}")
    
    # –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ
    print("\n2. –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ –Ω–∞ GPU...")
    z = torch.mm(x, y)
    print(f"   ‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç z: shape={z.shape}, device={z.device}")
    
    # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ CPU
    print("\n3. –ü–µ—Ä–µ–Ω–æ—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –Ω–∞ CPU...")
    z_cpu = z.cpu()
    print(f"   ‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ CPU: shape={z_cpu.shape}, device={z_cpu.device}")
    
    print("\n‚úÖ –í—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")


def benchmark_speed():
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å CPU vs GPU."""
    print("\n" + "=" * 60)
    print("–ë–ï–ù–ß–ú–ê–†–ö: CPU vs GPU")
    print("=" * 60)
    
    import time
    
    size = 5000
    iterations = 10
    
    # CPU
    print(f"\n‚è±Ô∏è  CPU –±–µ–Ω—á–º–∞—Ä–∫ ({iterations} –∏—Ç–µ—Ä–∞—Ü–∏–π, –º–∞—Ç—Ä–∏—Ü–∞ {size}x{size})...")
    x_cpu = torch.randn(size, size)
    y_cpu = torch.randn(size, size)
    
    start = time.time()
    for _ in range(iterations):
        z_cpu = torch.mm(x_cpu, y_cpu)
    cpu_time = time.time() - start
    print(f"   –í—Ä–µ–º—è CPU: {cpu_time:.2f} —Å–µ–∫—É–Ω–¥")
    
    # GPU
    print(f"\n‚ö° GPU –±–µ–Ω—á–º–∞—Ä–∫ ({iterations} –∏—Ç–µ—Ä–∞—Ü–∏–π, –º–∞—Ç—Ä–∏—Ü–∞ {size}x{size})...")
    x_gpu = torch.randn(size, size, device='cuda')
    y_gpu = torch.randn(size, size, device='cuda')
    
    # –ü—Ä–æ–≥—Ä–µ–≤ GPU
    for _ in range(3):
        _ = torch.mm(x_gpu, y_gpu)
    torch.cuda.synchronize()
    
    start = time.time()
    for _ in range(iterations):
        z_gpu = torch.mm(x_gpu, y_gpu)
    torch.cuda.synchronize()
    gpu_time = time.time() - start
    print(f"   –í—Ä–µ–º—è GPU: {gpu_time:.2f} —Å–µ–∫—É–Ω–¥")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
    speedup = cpu_time / gpu_time
    print(f"\nüöÄ –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {speedup:.2f}x")
    
    if speedup > 5:
        print("   ‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ! GPU —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ.")
    elif speedup > 2:
        print("   ‚ö†Ô∏è  –£–º–µ—Ä–µ–Ω–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ. –í–æ–∑–º–æ–∂–Ω–æ, –µ—Å—Ç—å —É–∑–∫–∏–µ –º–µ—Å—Ç–∞.")
    else:
        print("   ‚ùå –°–ª–∞–±–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ GPU.")


def test_model_training():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ GPU."""
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò")
    print("=" * 60)
    
    import torch.nn as nn
    import torch.optim as optim
    
    device = torch.device('cuda')
    
    # –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å
    class SimpleModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc1 = nn.Linear(1000, 512)
            self.fc2 = nn.Linear(512, 256)
            self.fc3 = nn.Linear(256, 10)
        
        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = torch.relu(self.fc2(x))
            x = self.fc3(x)
            return x
    
    print("\n1. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = SimpleModel().to(device)
    print(f"   ‚úì –ú–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {next(model.parameters()).device}")
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ loss
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    print("\n2. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–∞—Ç—á–∞—Ö...")
    for i in range(5):
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        x = torch.randn(32, 1000, device=device)
        y = torch.randint(0, 10, (32,), device=device)
        
        # Forward
        outputs = model(x)
        loss = criterion(outputs, y)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        print(f"   –ë–∞—Ç—á {i+1}/5: Loss = {loss.item():.4f}")
    
    print("\n‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ GPU!")


def check_memory_after_operations():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU."""
    print("\n" + "=" * 60)
    print("–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ü–ê–ú–Ø–¢–ò GPU")
    print("=" * 60)
    
    for i in range(torch.cuda.device_count()):
        memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
        memory_reserved = torch.cuda.memory_reserved(i) / 1024**3
        memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
        
        print(f"\nüéÆ GPU {i} ({torch.cuda.get_device_name(i)}):")
        print(f"  –í—ã–¥–µ–ª–µ–Ω–æ: {memory_allocated:.2f} GB")
        print(f"  –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ: {memory_reserved:.2f} GB")
        print(f"  –í—Å–µ–≥–æ: {memory_total:.2f} GB")
        print(f"  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {(memory_allocated/memory_total)*100:.1f}%")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    print("\n" + "üöÄ" * 30)
    print("–ü–†–û–í–ï–†–ö–ê GPU –î–õ–Ø PYTORCH")
    print("üöÄ" * 30)
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ PyTorch
    print(f"\nüì¶ –í–µ—Ä—Å–∏—è PyTorch: {torch.__version__}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
    if not check_cuda_available():
        print("\n" + "=" * 60)
        print("‚ùå –ó–ê–í–ï–†–®–ï–ù–ò–ï: CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        print("=" * 60)
        return
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ CUDA
    check_cuda_info()
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU
    check_gpu_info()
    
    # –¢–µ—Å—Ç—ã
    try:
        test_simple_operation()
        benchmark_speed()
        test_model_training()
        check_memory_after_operations()
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ç–µ—Å—Ç–æ–≤: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # –ò—Ç–æ–≥–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    print("\n" + "=" * 60)
    print("‚úÖ –í–°–ï –ü–†–û–í–ï–†–ö–ò –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û!")
    print("=" * 60)
    print("\n–í–∞—à–∞ RTX 4060 –≥–æ—Ç–æ–≤–∞ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π! üéâ")
    print("\n–î–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è:")
    print("  python scripts/train_model.py --model lightgcn --dataset movie_lens")
    print("\n–°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GPU.")
    print("–°–ª–µ–¥–∏—Ç–µ –∑–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPU —á–µ—Ä–µ–∑: nvidia-smi -l 1")
    print("=" * 60)


if __name__ == "__main__":
    main()

