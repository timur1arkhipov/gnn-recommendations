1) (orthogonal_bundle) Схема обучения эмбеддингов определена и зафиксирована:
   - user/item embeddings — обучаемые параметры модели.
   - Инициализация по умолчанию: случайная N(0, 0.01).
   - Основное обучение: end-to-end через BPR Loss (user/item embeddings + матрицы переноса + локальные трансформации оптимизируются совместно).
   - warm-start от LightGCN (как отдельный режим):
     1) обучить LightGCN на том же датасете,
     2) экспортировать user/item embeddings,
     3) загрузить embeddings в OrthogonalBundle и дообучить всю модель.
   - Для пользователей отсутствие признаков не проблема: их embeddings обучаются через взаимодействия (как в стандартных CF-моделях).
   команды:
   - python c:\Users\timur\OneDrive\Документы\diploma\gnn-recommendations\scripts\run_all_experiments.py --models lightgcn --datasets movie_lens --seed 42

   - python c:\Users\timur\OneDrive\Документы\diploma\gnn-recommendations\scripts\run_all_experiments.py --models orthogonal_bundle --datasets movie_lens --seed 42

1.2) объясни работу в LightGCN
   - Инициализация: обучаемые эмбеддинги пользователей и айтемов.
   - Графовая свёртка L раз: x <- A_norm * x (без нелинейностей и весов).
   - Layer aggregation: усреднение embeddings со всех слоёв (включая слой 0).
   - Скоринг: скалярное произведение user_emb и item_emb.
   - Обучение: BPR Loss, всё end‑to‑end.

2) (orthogonal_bundle) Архитектура формализована (вариант A, shared connection):
   - Соединительная матрица W^(l) зависит только от слоя l и является общей для всех рёбер.
   - W^(l) ортогональна и реализует общий transport между fiber spaces, что соответствует идее vector bundles.
   - Связь не зависит от конкретного ребра или эмбеддингов узлов → единые правила переноса по всему графу.
   - Формула слоя фиксируется явно:
     x^(l+1) = (1-α) · GS( PT(x^(l), W^(l)) ) + α · x^(0)
        x^(l) — эмбеддинги узлов на слое l.
        x^(0) — исходные (начальные) эмбеддинги (до всех слоёв).
        PT(·, W^(l)) — parallel transport: перенос информации от соседей с помощью ортогональной матрицы связи W^(l).
        W^(l) — общая ортогональная матрица для слоя l (shared для всех рёбер).
        GS(·) — Group & Shuffle: локальная ортогональная трансформация внутри fiber space (перемешивание/поворот признаков, без изменения нормы).
        α — коэффициент residual‑смешивания (обычно небольшой, например 0.1).
   - Число параметров считается прозрачно: embeddings + набор W^(l) + локальные ортогональные преобразования,
     и сравнивается с бейзлайнами.

3) (orthogonal_bundle) Валидация корректности: вводится runtime-метрика ортогональности.
   - На каждом N‑м шаге/эпохе логируется ||QᵀQ − I||_F для всех ортогональных матриц слоя.
   - Пороговая интерпретация: чем ближе к 0, тем лучше соблюдена ортогональность.
   - Дополнительно можно логировать max‑deviation по слоям (max |(QᵀQ − I)_ij|) для контроля численной стабильности.
   - Расшифровка строки лога:
     Orthogonality (lower is better) — все показатели ниже, если матрицы ближе к ортогональным.
     FroErr(local) — средняя ошибка ||QᵀQ − I||_F для локальных GS‑матриц.
     FroErr(conn) — средняя ошибка ||QᵀQ − I||_F для connection‑матриц W^(l).
     MaxDev(local) — максимальное по модулю отклонение элемента (QᵀQ − I) для GS.
     MaxDev(conn) — максимальное по модулю отклонение элемента (QᵀQ − I) для W^(l).
     Grade — качественная оценка по FroErr (меньше → лучше): совсем не ортогональные / слабая ортогональность / почти ортогональные / точно ортогональные.

4) методологические проблемы с экспериментами и сравнением. Пока модель не доведена до стабильной версии, сравнение с бейзлайнами некорректно. Неясно, как сравнивать модели в «равных условиях», если используется прогрев эмбеддингов или дополнительное предварительное обучение. Нет четкого плана: что является частью модели, а что — вспомогательной процедурой.
