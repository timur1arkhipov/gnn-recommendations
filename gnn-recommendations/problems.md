## 1 встреча:
1) (orthogonal_bundle) Схема обучения эмбеддингов определена и зафиксирована:
   - user/item embeddings — обучаемые параметры модели.
   - Инициализация по умолчанию: случайная N(0, 0.01).
   - Основное обучение: end-to-end через BPR Loss (user/item embeddings + матрицы переноса + локальные трансформации оптимизируются совместно).
   - warm-start от LightGCN (как отдельный режим):
     1) обучить LightGCN на том же датасете,
     2) экспортировать user/item embeddings,
     3) загрузить embeddings в OrthogonalBundle и дообучить всю модель.
   - Для пользователей отсутствие признаков не проблема: их embeddings обучаются через взаимодействия (как в стандартных CF-моделях).
   команды:
   - python c:\Users\timur\OneDrive\Документы\diploma\gnn-recommendations\scripts\run_all_experiments.py --models lightgcn --datasets movie_lens --seed 42

   - python c:\Users\timur\OneDrive\Документы\diploma\gnn-recommendations\scripts\run_all_experiments.py --models orthogonal_bundle --datasets movie_lens --seed 42

1.2) LightGCN
   - Инициализация: обучаемые эмбеддинги пользователей и айтемов.
   - Графовая свёртка L раз: x <- A_norm * x (без нелинейностей и весов).
   - Layer aggregation: усреднение embeddings со всех слоёв (включая слой 0).
   - Скоринг: скалярное произведение user_emb и item_emb.
   - Обучение: BPR Loss, всё end‑to‑end.

2) (orthogonal_bundle) Архитектура формализована (вариант A, shared connection):
   - Соединительная матрица W^(l) зависит только от слоя l и является общей для всех рёбер.
   - W^(l) ортогональна и реализует общий transport между fiber spaces, что соответствует идее vector bundles.
   - Связь не зависит от конкретного ребра или эмбеддингов узлов → единые правила переноса по всему графу.
   - Формула слоя фиксируется явно:
     x^(l+1) = (1-α) · GS( PT(x^(l), W^(l)) ) + α · x^(0)
        x^(l) — эмбеддинги узлов на слое l.
        x^(0) — исходные (начальные) эмбеддинги (до всех слоёв).
        PT(·, W^(l)) — parallel transport: перенос информации от соседей с помощью ортогональной матрицы связи W^(l).
        W^(l) — общая ортогональная матрица для слоя l (shared для всех рёбер).
        GS(·) — Group & Shuffle: локальная ортогональная трансформация внутри fiber space (перемешивание/поворот признаков, без изменения нормы).
        α — коэффициент residual‑смешивания (обычно небольшой, например 0.1).
   - Число параметров считается прозрачно: embeddings + набор W^(l) + локальные ортогональные преобразования,
     и сравнивается с бейзлайнами.

3) (orthogonal_bundle) Валидация корректности: вводится runtime-метрика ортогональности.
   - На каждом N‑м шаге/эпохе логируется ||QᵀQ − I||_F для всех ортогональных матриц слоя.
   - Пороговая интерпретация: чем ближе к 0, тем лучше соблюдена ортогональность.
   - Дополнительно можно логировать max‑deviation по слоям (max |(QᵀQ − I)_ij|) для контроля численной стабильности.
   - Расшифровка строки лога:
     Orthogonality (lower is better) — все показатели ниже, если матрицы ближе к ортогональным.
     FroErr(local) — средняя ошибка ||QᵀQ − I||_F для локальных GS‑матриц.
     FroErr(conn) — средняя ошибка ||QᵀQ − I||_F для connection‑матриц W^(l).
     MaxDev(local) — максимальное по модулю отклонение элемента (QᵀQ − I) для GS.
     MaxDev(conn) — максимальное по модулю отклонение элемента (QᵀQ − I) для W^(l).
     Grade — качественная оценка по FroErr (меньше → лучше): совсем не ортогональные / слабая ортогональность / почти ортогональные / точно ортогональные.

4) методологические проблемы с экспериментами и сравнением. Пока модель не доведена до стабильной версии, сравнение с бейзлайнами некорректно. Неясно, как сравнивать модели в «равных условиях», если используется прогрев эмбеддингов или дополнительное предварительное обучение. Нет четкого плана: что является частью модели, а что — вспомогательной процедурой.

Вопросы:
1) если у меня датасеты explicit и implicit, но я по сути привожу их к implicit (рейтинг >= 4 = 1, рейтинг < 4 = 0), насколько это верно? м.б. стоит другой способ подобрать?
2) как верно будет сравнивать модели особенно при учете что в orthogonal bundle есть warm-up с light gcn ?
3) деление дата сета random per-user сейчас, нормальный ли это подход?
4) по проведению экспериментов, сейчас 3-datasets, 6 baselines + orthogonal bundle, 5 seeds и 10 метрик, хватит ли это?
5) вот проблема с размером датасета, на примере movielens, какой минимуальный размер?

## 2 встреча
1) Архитектура и сравнение с LightGCN
– LightGCN можно рассматривать как базовую архитектуру для прогрева эмбеддингов.
– Корректный подход: сначала обучить модель как LightGCN (без дополнительных преобразований), затем в той же архитектуре заменить свёртку/месседж-пассинг на свою.
– Размерности слоёв и полносвязные слои не менять, чтобы сравнение было честным.
– Эмбеддинги можно замораживать или дообучать; даже на замороженных ожидается улучшение.
– Сравнение «одна и та же архитектура + разная свёртка» считается корректным.

2) Стратегия по бейзлайнам:
Не фиксировать параметры, а максимизировать метрику при доступных ресурсах.
– Для диплома допустимо ориентироваться на метрики из статей, а не обязательно полностью воспроизводить чужой код.
– Желательно брать датасеты и постановку (split, метрики), используемые в релевантных статьях.

3) Датасеты
– Малые датасеты (например, MovieLens 100k) подходят только для отладки пайплайна.
– Для реальных сравнений лучше брать крупные датасеты, иначе простые методы (SVD) могут выглядеть лучше нейросетей.
– Лучше использовать те же датасеты, что и в статьях, с которыми идёт сравнение (MovieLens 1M/20M/33M, Yahoo, Facebook Box и т.д.).

4) Train / test split - скопировать из sheaf статьи для верного сравнения.