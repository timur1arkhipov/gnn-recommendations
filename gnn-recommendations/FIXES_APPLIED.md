# Исправления проблем обучения GNN моделей

## Дата: 09.01.2026

## Обнаруженные проблемы

### 1. Все GNN модели показывали идентичные результаты
- **Проблема**: LightGCN, GCNII, DGR, SVD-GCN, LayerGCN, GroupShuffle показывали одинаковые метрики
- **Причина**: Модели не обучались должным образом, застревали в локальном минимуме

### 2. GNN модели значительно хуже простого BPR-MF
- **Проблема**: BPR-MF показывал результаты в 3-4 раза лучше всех GNN моделей
- **MovieLens**: BPR-MF Recall@10 = 0.0232 vs GNN Recall@10 = 0.0059
- **Причина**: Неправильные гиперпараметры обучения

### 3. Катастрофически низкий Coverage
- **Проблема**: GNN модели рекомендовали только 1.2% каталога (vs 19.3% у BPR-MF)
- **Причина**: Модели не обучались и выдавали одинаковые рекомендации

### 4. Ранняя остановка на epoch 1
- **Проблема**: Большинство GNN моделей останавливались после 1 эпохи
- **Причина**: Слишком строгий early stopping и проблемы с обучением

## Примененные исправления

### 1. ✅ Уменьшен Learning Rate для GNN моделей
**Файлы**: `config/models/*.yaml`

```yaml
# Было:
learning_rate: 0.001

# Стало:
learning_rate: 0.0001  # Уменьшено в 10 раз для стабильности GNN
```

**Обоснование**: GNN модели более чувствительны к learning rate из-за графовой свертки. Слишком большой LR приводит к нестабильности градиентов.

### 2. ✅ Увеличен init_scale для лучшей инициализации
**Файлы**: `config/models/*.yaml`

```yaml
# Было:
init_scale: 0.01

# Стало:
init_scale: 0.1  # Увеличено в 10 раз
```

**Обоснование**: Слишком маленькая инициализация (0.01) приводит к очень маленьким градиентам в начале обучения, модель не может "выйти" из начального состояния.

### 3. ✅ Уменьшен weight_decay
**Файлы**: `config/models/*.yaml`

```yaml
# Было:
weight_decay: 1e-4

# Стало:
weight_decay: 1e-5  # Уменьшено в 10 раз
```

**Обоснование**: Слишком сильная регуляризация мешает обучению GNN моделей.

### 4. ✅ Увеличен patience для early stopping
**Файлы**: `config/models/*.yaml`

```yaml
# Было:
patience: 20

# Стало:
patience: 50  # Увеличено в 2.5 раза
```

**Обоснование**: GNN моделям нужно больше времени для обучения, особенно с меньшим learning rate.

### 5. ✅ Уменьшен min_delta для early stopping
**Файлы**: `config/models/*.yaml`

```yaml
# Было:
min_delta: 0.0001

# Стало:
min_delta: 0.00001  # Уменьшено в 10 раз
```

**Обоснование**: Более чувствительное определение улучшения метрик.

### 6. ✅ Добавлен Gradient Clipping
**Файл**: `src/training/trainer.py`

```python
# Добавлено в __init__:
self.max_grad_norm = float(config.get('max_grad_norm', 1.0))

# Добавлено в train_epoch после backward:
if self.max_grad_norm > 0:
    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
```

**Обоснование**: Предотвращает exploding gradients в глубоких GNN.

### 7. ✅ Исправлена логика обновления embeddings
**Файл**: `src/training/trainer.py`

```python
# Было:
if batch_idx % 100 == 0:  # Обновление каждые 100 батчей
    with torch.no_grad():
        user_emb, item_emb = self.model(adj_matrix)

# Стало:
# Обновляем после КАЖДОГО батча
with torch.no_grad():
    user_emb, item_emb = self.model(adj_matrix)
```

**Обоснование**: Это была КРИТИЧЕСКАЯ ошибка! Модель обновляла параметры, но использовала старые embeddings для следующих батчей. Из-за этого модель фактически не обучалась.

### 8. ✅ Добавлен Learning Rate Warmup
**Файл**: `src/training/trainer.py`

```python
# Добавлен warmup scheduler:
self.warmup_epochs = int(config.get('warmup_epochs', 5))
self.scheduler = optim.lr_scheduler.CosineAnnealingLR(...)

# В цикле обучения:
if epoch <= self.warmup_epochs:
    warmup_lr = self.base_lr * (epoch / self.warmup_epochs)
    for param_group in self.optimizer.param_groups:
        param_group['lr'] = warmup_lr
elif self.scheduler is not None:
    self.scheduler.step()
```

**Обоснование**: Постепенное увеличение learning rate в начале обучения стабилизирует процесс.

### 9. ✅ Добавлено логирование learning rate
**Файл**: `src/training/trainer.py`

```python
# Теперь выводится текущий LR:
print(f"Epoch {epoch:3d}/{self.epochs} | "
      f"LR: {current_lr:.6f} | "  # Новое!
      f"Train Loss: {train_loss:.4f} | "
      f"Recall@10: {current_metric:.4f} | "
      f"NDCG@10: {valid_metrics.get('ndcg@10', 0.0):.4f}")
```

## Новые параметры в конфигурациях

Все GNN модели теперь имеют следующие параметры:

```yaml
# Параметры модели
model:
  init_scale: 0.1  # Увеличено

# Параметры обучения
training:
  learning_rate: 0.0001  # Уменьшено
  weight_decay: 1e-5  # Уменьшено
  early_stopping:
    patience: 50  # Увеличено
    min_delta: 0.00001  # Уменьшено
  
  # Новые параметры:
  use_scheduler: true
  warmup_epochs: 5
  max_grad_norm: 1.0
```

## Ожидаемые результаты

После применения этих исправлений ожидается:

1. **Разные результаты для разных моделей** - каждая архитектура должна показывать свои уникальные результаты
2. **GNN модели сопоставимы или лучше BPR-MF** - как минимум не должны быть в 4 раза хуже
3. **Более высокий Coverage** - должен быть близок к BPR-MF (15-20%)
4. **Более долгое обучение** - модели должны обучаться 20-100 эпох, а не останавливаться на 1-й

## Как запустить повторное обучение

```bash
cd gnn-recommendations
python scripts/run_all_experiments.py
```

Или для отдельной модели:

```bash
python scripts/train_model.py --model lightgcn --dataset movie_lens
```

## Примечания

- **BPR-MF конфигурация не изменена** - она уже работала хорошо
- **Датасет Gowalla** - всё ещё слишком большой для RTX 4060 (8GB). Рекомендуется:
  - Использовать только MovieLens и Book Crossing
  - Или добавить сэмплирование для Gowalla
  - Или использовать CPU (будет медленно)

## Технические детали

### Почему GNN требуют особого подхода?

1. **Графовая свертка усиливает сигнал** - каждый слой умножает на adjacency matrix, что может привести к exploding gradients
2. **Глубокие сети** - 3 слоя GCN эквивалентны более глубокой обычной сети
3. **Sparse операции** - работа с разреженными матрицами требует аккуратной работы с памятью
4. **Oversmoothing** - слишком много слоев приводит к тому, что все embeddings становятся одинаковыми

### Почему обновление embeddings критично?

В GNN модели параметры (веса) и embeddings - это разные вещи:
- **Параметры** обновляются через `optimizer.step()`
- **Embeddings** вычисляются через `model(adj_matrix)` на основе параметров

Если не обновлять embeddings после каждого батча, то:
1. Параметры меняются
2. Но для вычисления loss используются старые embeddings
3. Градиенты вычисляются неправильно
4. Модель не обучается

Это объясняет, почему все модели показывали одинаковые результаты!

## Следующие шаги

1. Запустить обучение с новыми параметрами
2. Сравнить результаты с предыдущими
3. При необходимости провести дополнительную настройку гиперпараметров
4. Добавить поддержку больших датасетов (Gowalla) через сэмплирование или CPU fallback

